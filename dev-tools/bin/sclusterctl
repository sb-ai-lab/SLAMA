#!/bin/bash

set -ex

# build settings
BASE_IMAGE_NAME=node2.bdcl:5000/spark-base:3.2.1-py3.9
MASTER_WORKER_IMAGE_NAME=node2.bdcl:5000/spark-master-worker:3.2.1-py3.9
SUBMITTER_IMAGE_NAME=node2.bdcl:5000/spark-submitter:3.2.1-py3.9
DOCKERFILE_FOLDER=dev-tools/spark-cluster-lama/dockerfiles/base
BUILD_FOLDER=dev-tools/spark-cluster-lama/dockerfiles/base
BUILD_TMP_FOLDER=${BUILD_FOLDER}/build_tmp

# deploy settings
BASE_REMOTE_DIR="/mnt/nfs/scluster"
COMPOSE_DIR="${BASE_REMOTE_DIR}/spark-cluster-lama"
control_node="node3.bdcl"
master="node3.bdcl"
workers=()

# node20 experiences some problems
# node6 docker settings are inappropriate (see /etc/docker/daemon.json)
ignored_nodes=(6 20)
for (( node=4; node<=21; node++ ))
do
    if [[ ! " ${ignored_nodes[*]} " =~ ${node} ]]; then
        workers+=("node${node}.bdcl")
    else
        echo "Won't add node ${node} to the workers list because it is in the ignored nodes list"
    fi
done

function build_wheel() {
  poetry export -f requirements.txt > requirements.txt
  poetry build
}

function build_images() {
  local do_build_dist="${1:-false}"    # Default value is false

  if "${do_build_dist}"; then
    build_wheel
  fi

  rm -rf ${BUILD_TMP_FOLDER}
  mkdir ${BUILD_TMP_FOLDER}
  cp requirements.txt ${BUILD_TMP_FOLDER}
  cp -r dist ${BUILD_TMP_FOLDER}/dist
  cp -r examples ${BUILD_TMP_FOLDER}/examples
  cp -r dev-tools/experiments ${BUILD_TMP_FOLDER}/experiments
  cp jars/spark-lightautoml_2.12-0.1.jar ${BUILD_TMP_FOLDER}/spark-lightautoml_2.12-0.1.jar

  docker build -t ${BASE_IMAGE_NAME} -f ${DOCKERFILE_FOLDER}/base.dockerfile ${BUILD_FOLDER}
  docker push ${BASE_IMAGE_NAME}

  docker build -t ${MASTER_WORKER_IMAGE_NAME} -f ${DOCKERFILE_FOLDER}/spark-master-worker.dockerfile ${BUILD_FOLDER}
  docker push ${MASTER_WORKER_IMAGE_NAME}

  docker build -t ${SUBMITTER_IMAGE_NAME} -f ${DOCKERFILE_FOLDER}/spark-submitter.dockerfile ${BUILD_FOLDER}
  docker push ${SUBMITTER_IMAGE_NAME}

  rm -rf ${BUILD_TMP_FOLDER}
}

function create_config_on_control_node() {
  ssh ${control_node} "mkdir -p ${BASE_REMOTE_DIR} && rm -rf ${BASE_REMOTE_DIR}/spark-cluster-lama"
  scp -r dev-tools/spark-cluster-lama ${control_node}:${COMPOSE_DIR}
}

function deploy_workers() {
  local force_mode="${1:-false}"

  if "${force_mode}"; then
    args="--force-recreate"
  else
      args=""
  fi

  for host in "${workers[@]}"
  do
      echo "Starting a worker on host: ${host}"
      # shellcheck disable=SC2029
      ssh $host "cd ${COMPOSE_DIR} && docker-compose pull && docker-compose up -d ${args} spark-worker-1"
  done
}

function deploy_scluster() {
  local force_mode="${1:-false}"    # Default value is false

  if "${force_mode}"; then
      args="--force-recreate"
  else
      args=""
  fi

  echo "Starting the master on host: ${master}"
  # shellcheck disable=SC2029
  ssh ${master} "cd ${COMPOSE_DIR} && docker-compose pull && docker-compose up -d ${args} spark-master spark-submit"

  deploy_workers ${force_mode}
}

function teardown_workers() {
  for host in "${workers[@]}"
  do
      echo "Stopping a worker on host: ${host}"
      # shellcheck disable=SC2029
      ssh $host "cd ${COMPOSE_DIR} && docker-compose down --remove-orphans"
  done
}

function teardown_scluster() {
  teardown_workers
  
  echo "Stopping the master on host: ${master}"
  # shellcheck disable=SC2029
  ssh ${master} "cd ${COMPOSE_DIR} && docker-compose down"
}

function run() {
    docker run -d \
     -v /mnt/ess_storage/DN_1/storage/SLAMA/kaggle_used_cars_dataset:/opt/spark_data \
     -v /mnt/nfs/spark-lama-pipelines:/tmp/spark_results \
     --env EXEC_CORES="${EXEC_CORES:-6}" --env EXEC_INST="${EXEC_INST:-8}" \
     --env EXEC_MEM="${EXEC_MEM:-110g}" --env DS_INC="${DS_INC:-1}" \
     --network host ${SUBMITTER_IMAGE_NAME} \
     /examples/experiments/tmp-le-scaling.py
#     /examples/experiments/tmp-tabular-preset-automl.py
#     /examples/experiments/calc-func.py
#     /examples/experiments/tabular-preset-automl.py
}

function run_lama() {
    docker run -d \
     -v /mnt/ess_storage/DN_1/storage/SLAMA/kaggle_used_cars_dataset:/opt/spark_data \
     --env EXEC_CORES="${EXEC_CORES:-6}" \
     --env EXEC_MEM="${EXEC_MEM:-128}" \
     --env DS_NAME="used_cars_dataset_4x" \
     --cpuset-cpus "0-19" \
     --network host \
     --entrypoint="python3" \
     ${SUBMITTER_IMAGE_NAME} \
     /examples/spark/tmp-lama-tabular-preset.py
}

function help() {
  echo "
  List of commands.
    build-wheel - builds SLAMA wheel
    build-images - builds SLAMA wheel and builds and pushes docker images
    build-images-no-poetry - builds and pushes docker images only
    create-config -  create docker-compose file remotely on control node (${control_node})
    deploy - deploy spark cluster (master will be on ${master})
    deploy-workers - deploy some spark workers (without force)
    redeploy - forcefully deploy spark cluster (e.g. to apply new config)
    teardown-workers - stop some spark workers
    teardown - stop spark cluster
    run - execute
    run-lama - execute local lama run
    help - prints this message
  "
}


function main () {
    cmd="$1"

    if [ -z "${cmd}" ]
    then
      echo "No command is provided."
      help
      exit 1
    fi

    shift 1

    echo "Executing command: ${cmd}"

    case "${cmd}" in
    "build-wheel")
        build_wheel
        ;;

    "build-images")
        build_images true
        ;;

    "build-images-no-poetry")
        build_images false
        ;;

    "create-config")
        create_config_on_control_node
        ;;

    "deploy")
        deploy_scluster false
        ;;

    "deploy-workers")
        deploy_workers
        ;;

    "redeploy")
        deploy_scluster true
        ;;

    "teardown-workers")
        teardown_workers
        ;;

    "teardown")
        teardown_scluster
        ;;

    "run")
        run
        ;;

    "run-lama")
        run_lama
        ;;

    "help")
        help
        ;;

    *)
        echo "Unknown command: ${cmd}"
        ;;

    esac
}

main "${@}"
